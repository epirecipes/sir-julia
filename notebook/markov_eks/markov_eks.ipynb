{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Fitting a stochastic, discrete-time Markov model using Ensemble Kalman Sampling in EnsembleKalmanProcesses.jl\nSimon Frost (@sdwfrost), 2023-03-16\n\n## Introduction\n\nFitting stochastic models to data is more challenging than fitting deterministic models. This example uses [`EnsembleKalmanProcesses.jl`](https://github.com/CliMA/EnsembleKalmanProcesses.jl) to fit a stochastic, discrete-time Markov model to simulated data using an [Ensemble Kalman Sampler](https://epubs.siam.org/doi/10.1137/19M1251655), which returns an approximate posterior distribution of parameter estimates.\n\n## Libraries"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "using OrdinaryDiffEq\nusing EnsembleKalmanProcesses\nusing EnsembleKalmanProcesses.ParameterDistributions\nconst EKP = EnsembleKalmanProcesses\nusing Random\nusing LinearAlgebra # Provides identity matrix `I`\nusing Distributions\nusing ThreadsX\nusing Plots\nusing StatsPlots;"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transitions\n\nWe use a stochastic, discrete-time Markov model to describe the dynamics. The following model assumes that the time step, `dt` is 1, and takes 10 smaller steps (with `δt=0.1`) within the function. This allows us to reset the number of cases, `C` to zero each time step, without having to use callbacks or the SciML integrator interface. We scale the infectivity parameter β by dividing by the population size, so that is is on approximately the same scale as γ and I₀."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "function sir_markov(u,p,t)\n    (S, I, C) = u\n    C = 0\n    (β, γ, N) = p\n    δt = 0.1\n    nsteps = 10\n    for i in 1:nsteps\n        ifrac = 1-exp(-β*I/N*δt)\n        rfrac = 1-exp(-γ*δt)\n        infection = rand(Binomial(S,ifrac))\n        recovery = rand(Binomial(I,rfrac))\n        S = S-infection\n        I = I+infection-recovery\n        C = C+infection\n    end\n   [S, I, C]\nend;"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Time, initial conditions, and parameter values"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "tspan = (0,40)\nu0 = [990, 10, 0] # S, I, C\nβ = 0.5\nγ = 0.25\nN = 1000\ni₀ = 0.01\np = [β, γ, N]\nseed = 1234;"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Running the model\n\nWe perform a single run of the model to illustrate the dynamics and to obtain simulated data. We will base inference on either (a) summary statistics (the peak number of cases, the time of the peak, and the total number of cases over the period `tspan`) or (b) on the number of new cases per day, `C`. For the latter, we calculate log(C+1) in order to generate data that more closely approximates a multivariate Gaussian distribution."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "Random.seed!(seed)\nprob = DiscreteProblem(sir_markov, u0, tspan, p)\nsol = solve(prob, FunctionMap())\nplot(sol, labels=[\"S\" \"I\" \"C\"], xlabel=\"Time\", ylabel=\"Number\")"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "C = Float64.(hcat(sol.u...)[3,2:end])\nsummary_stats = [maximum(C), Float64(argmax(C)),  sum(C)]\ncases = log.(C .+ 1);"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generating simulated data"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "function get_summary_stats(q)\n    i0 = Float64(round(N*q[2]))\n    problem = remake(prob, p=[q[1], γ, N],u0=[N-i0,i0,0.0])\n    sol = solve(problem, FunctionMap())\n    C = Float64.(hcat(sol.u...)[3,2:end])\n    return [maximum(C), Float64(argmax(C)),  sum(C)]\nend;"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "function get_cases(q)\n    i0 = Float64(round(N*q[2]))\n    problem = remake(prob, p=[q[1], γ, N],u0=[N-i0,i0,0.0])\n    sol = solve(problem, FunctionMap())\n    C = Float64.(hcat(sol.u...)[3,2:end])\n    return log.(C .+ 1)\nend;"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Priors\n\nA `constrained_gaussian(\"name\", 0.5, 0.3, 0.0, 1.0)` will give a prior close to a uniform variable between 0 and 1; below, we transform this to give approximate uniform distributions for β (between 0 and 1) and I₀ (between 0 and 0.1)."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "prior_u1 = constrained_gaussian(\"β\", 0.5, 0.3, 0.0, 1.0)\nprior_u2 = constrained_gaussian(\"i₀\", 0.05, 0.03, 0.0, 0.1)\nprior = combine_distributions([prior_u1, prior_u2]);"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ensemble Kalman Sampler\n\n### Initialization"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "Γ = 1e-4 * LinearAlgebra.I\nprior_mean = mean(prior)\nprior_cov = cov(prior)\neks_process = Sampler(prior_mean, prior_cov);"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Inference based on summary statistics"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "N_iterations_ss = 50\nN_ensemble_ss = 1000\nrng_ss = Random.Xoshiro(seed)\ninitial_ensemble_ss = EKP.construct_initial_ensemble(rng_ss, prior, N_ensemble_ss);"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "eks_obj_ss = EKP.EnsembleKalmanProcess(initial_ensemble_ss, summary_stats, Γ, eks_process; rng=rng_ss)\nfor i in 1:N_iterations_ss\n    params_i = get_ϕ_final(prior, eks_obj_ss)\n    # Without threads would be as follows\n    # ss = hcat([get_summary_stats(params_i[:, i]) for i in 1:N_ensemble_ss]...)\n    ss = hcat(ThreadsX.collect(get_summary_stats(params_i[:, i]) for i in 1:N_ensemble_ss)...)\n    EKP.update_ensemble!(eks_obj_ss, ss)\nend;"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following generate the mean, θ, and empirical covariance, Γ, of the parameters in unconstrained space."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "θ_post_ss = get_u_mean_final(eks_obj_ss)\nΓ_post_ss = get_u_cov_final(eks_obj_ss);"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we draw random samples from the unconstrained parameter space and transform to the constrained scale."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "post_samples_ss_unconstrained = rand(MvNormal(θ_post_ss, Γ_post_ss), 10000)\npost_samples_ss = transform_unconstrained_to_constrained(prior, post_samples_ss_unconstrained);"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here are the quantiles of the posterior distribution, first for β."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "quantile(post_samples_ss[1,:], [0.025, 0.5, 0.975])"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next for i₀."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "quantile(post_samples_ss[2,:], [0.025, 0.5, 0.975])"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "corrplot(post_samples_ss',labels=[\"β\" \"i₀\"])"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can now compare the prior distribution with the (approximate) posterior distribution."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "prior_ensemble_ss = get_ϕ(prior, eks_obj_ss, 1);"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "l = @layout [a b; c d]\np1 = histogram(prior_ensemble_ss[1,:], legend=false, title=\"β\", xlim=(0, 1.0), bins=0:0.01:1.0)\np2 = histogram(prior_ensemble_ss[2,:], legend=false, title=\"i₀\", xlim=(0, 0.1), bins=0:0.001:0.1)\np3 = histogram(post_samples_ss[1,:], legend=false, title=\"β\", xlim=(0, 1.0), bins=0:0.01:1.0)\np4 = histogram(post_samples_ss[2,:], legend=false, title=\"i₀\", xlim=(0, 0.1), bins=0:0.001:0.1)\nplot(p1, p3, p2, p4, layout=l)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Inference based on case time series\n\n\nWe repeat the above, but now for case data, which is of higher dimension (40) compared to the summary statistics (3). Correspondingly, we set the number of iterations and the number of ensemble members higher."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "N_iterations_cs = 50\nN_ensemble_cs = 10000\nrng_cs = Random.Xoshiro(seed)\ninitial_ensemble_cs = EKP.construct_initial_ensemble(rng_cs, prior, N_ensemble_cs);"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "eks_obj_cases = EKP.EnsembleKalmanProcess(initial_ensemble_cs, cases, Γ, eks_process; rng = rng_cs)\nfor i in 1:N_iterations_cs\n    params_i = get_ϕ_final(prior, eks_obj_cases)\n    # cs = hcat([get_cases(params_i[:, i]) for i in 1:N_ensemble_cs]...)\n    cs = hcat(ThreadsX.collect(get_cases(params_i[:, i]) for i in 1:N_ensemble_cs)...)\n    EKP.update_ensemble!(eks_obj_cases, cs)\nend"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "θ_post_cases = get_u_mean_final(eks_obj_cases)\nΓ_post_cases = get_u_cov_final(eks_obj_cases);"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "post_samples_cases_unconstrained = rand(MvNormal(θ_post_cases, Γ_post_cases), 10000)\npost_samples_cases = transform_unconstrained_to_constrained(prior, post_samples_cases_unconstrained);"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here are the quantiles of the posterior distribution of β."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "quantile(post_samples_cases[1,:], [0.025, 0.5, 0.975])"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next for i₀."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "quantile(post_samples_cases[2,:], [0.025, 0.5, 0.975])"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "corrplot(post_samples_cases',labels=[\"β\" \"i₀\"])"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "prior_ensemble_cases = get_ϕ(prior, eks_obj_cases, 1);"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, we plot the prior and posterior distribution of the ensemble."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "l = @layout [a b; c d]\np1 = histogram(prior_ensemble_cases[1,:], legend=false, title=\"β\", xlim=(0, 1.0), bins=0:0.01:1.0)\np2 = histogram(prior_ensemble_cases[2,:], legend=false, title=\"i₀\", xlim=(0, 0.1), bins=0:0.001:0.1)\np3 = histogram(post_samples_cases[1,:], legend=false, title=\"β\", xlim=(0, 1.0), bins=0:0.01:1.0)\np4 = histogram(post_samples_cases[2,:], legend=false, title=\"i₀\", xlim=(0, 0.1), bins=0:0.001:0.1)\nplot(p1, p3, p2, p4, layout=l)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Discussion\n\nUnlike the corresponding notebook that uses Ensemble Kalman Inversion to obtain point estimates, the use of uncertainty quantification such as through Ensemble Kalman Sampling allows us to capture the loss of information when we go from the full time series of cases to summary statistics."
      ],
      "metadata": {}
    }
  ],
  "nbformat_minor": 2,
  "metadata": {
    "language_info": {
      "file_extension": ".jl",
      "mimetype": "application/julia",
      "name": "julia",
      "version": "1.8.5"
    },
    "kernelspec": {
      "name": "julia-1.8",
      "display_name": "Julia 1.8.5",
      "language": "julia"
    }
  },
  "nbformat": 4
}
