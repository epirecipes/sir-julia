{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Inference for a stochastic Markov model using Gen.jl\nSimon Frost (@sdwfrost), 2024-07-15\n\n## Introduction\n\n[`Gen.jl`](https://www.gen.dev) is a probabilistic programming language that allows for the definition of generative models and the inference of parameters from data. In this notebook, we will use `Gen.jl` to infer the parameters of an SIR model (as a discrete-time Markov model) from simulated data using Importance Sampling (IS), Markov Chain Monte Carlo (MCMC), and Sequential Monte Carlo (SMC). The problem specification is similar to that of the [ODE example in Gen.jl](https://github.com/epirecipes/sir-julia/blob/master/markdown/ode_gen/ode_gen.md), and illustrates that relatively minor changes are needed in order to fit a stochastic model.\n\n## Libraries"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "using OrdinaryDiffEq\nusing SciMLSensitivity\nusing Distributions\nusing Random\nusing Gen\nusing GenDistributions # to define custom distributions\nusing GenParticleFilters # for SMC\nusing Plots;"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "We set a fixed seed for reproducibility."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "Random.seed!(1234);"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model specification\n\nThe first argument is the number of timesteps to generate the data, in the form of new cases per day; this convention is important for running Sequential Monte Carlo. Random variables are declared using `~`; for array variables, we use the syntax `(:y, i)` to declare the address of the variable `y` at index `i`. There is no measurement noise in this model; the stochasticity is from the infection and recovery processes, as well as from the priors for the infection rate, `β`, and the initial number of infected individuals, `i₀` (note that `i₀` is used to denote the initial *proportion* of infected individuals in the ODE tutorial). For simplicity, the timestep of the model and the data are defined to be the same. Fixed parameters, such as the contact rate, `c`, and the recovery rate, `γ`, are passed as arguments."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "@gen function sir_markov_model(l::Int=40,\n                               N::Int=1000,\n                               c::Float64=10.0,\n                               γ::Float64=0.25,\n                               δt::Float64=1.0)\n    i₀ ~ uniform_discrete(1, 100)\n    β ~ uniform(0.01, 0.1)\n    S = N - i₀\n    I = i₀\n    for i in 1:l\n        ifrac = 1-exp(-β*c*I/N*δt)\n        rfrac = 1-exp(-γ*δt)\n        infection = {(:y, i)} ~ binom(S,ifrac)\n        recovery = {(:z, i)} ~ binom(I,rfrac)\n        S = S-infection\n        I = I+infection-recovery\n    end\n    nothing\nend;"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Simulating data\n\nWe simulate data from the model, by constraining two of the stochastic nodes representing the parameters. The following generates 40 timepoints of data. Note the use of `(l,fixed_args...)`; this avoids the use of default arguments within the function, and splitting the arguments into two will allow the use of SMC later on."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "p = Gen.choicemap()\np[:β] = 0.05\np[:i₀] = 10\nfixed_args = (1000, 10.0, 0.25, 1.0)\nl = 40\n(sol, _) = Gen.generate(sir_markov_model, (l,fixed_args...), p);"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can verify the arguments passed to the model, as well as extract the values of the parameters, as follows."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "Gen.get_args(sol)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "sol[:β]"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "sol[:i₀]"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can extract the simulated number of cases at index `i`, `(:y, i)`, and plot as follows."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "ts = collect(range(1,l))\nY = [sol[(:y, i)] for i=1:l]\nscatter(ts,Y,xlabel=\"Time\",ylabel=\"Number\",label=false)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inference\n\nIn order to perform inference, we constrain the observations (given by the addresses `(:y, i)` to the simulated data, `Y`."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "observations = Gen.choicemap()\nfor (i, y) in enumerate(Y)\n    observations[(:y, i)] = y\nend;"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Importance sampling\n\nFor importance sampling, we use `Gen.importance_resampling`, a function that takes the model, the observations, and the number of particles, and returns the trace of the model. We can then extract the parameters of interest from the trace, which we store in a `Vector`. Note that the only difference between this and the ODE version is that we have a different type (`Int` rather than `Real`) for the initial number of infected individuals, `i₀`."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "num_particles = 1000\nnum_replicates = 1000\nβ_is = Vector{Real}(undef, num_replicates)\ni₀_is = Vector{Int}(undef, num_replicates)\nfor i in 1:num_replicates\n    (trace, lml_est) = Gen.importance_resampling(sir_markov_model, (l,fixed_args...), observations, num_particles)\n    β_is[i] = trace[:β]\n    i₀_is[i] = trace[:i₀]\nend;"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can then calculate the mean and standard deviation of the inferred parameters, as well as plot out marginal histograms."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "mean(β_is),sqrt(var(β_is))"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "mean(i₀_is),sqrt(var(i₀_is))"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "pl_β_is = histogram(β_is, label=false, title=\"β\", ylabel=\"Density\", density=true, xrotation=45)\nvline!([sol[:β]], label=\"True value\")\npl_i₀_is = histogram(i₀_is, label=false, title=\"i₀\", ylabel=\"Density\", density=true, xrotation=45)\nvline!([sol[:i₀]], label=\"True value\")\nplot(pl_β_is, pl_i₀_is, layout=(1,2), plot_title=\"Importance sampling\")"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Metroplis-Hastings Markov Chain Monte Carlo\n\nIn order to guide the Metropolis-Hastings algorithm, we define a proposal distribution. In order to keep the parameter estimates positive, we use truncated normal distributions. These are not defined in Gen.jl, so we define a new distribution type using `DistributionsBacked` from `GenDistributions.jl`."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "const truncated_normal = DistributionsBacked((mu,std,lb,ub) -> Distributions.Truncated(Normal(mu, std), lb, ub),\n                                             (true,true,false,false),\n                                             true,\n                                             Float64)\n\n\n@gen function sir_proposal(current_trace)\n    β ~ truncated_normal(current_trace[:β], 0.01, 0.0, Inf)\n    i₀ ~ uniform_discrete(current_trace[:i₀]-1, current_trace[:i₀]+1)\nend;"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can then run the Metropolis-Hastings algorithm, storing the parameter estimates in `β_mh` and `i₀_mh`, and the scores in `scores`. If we wanted to omit the targeted proposal, we could use `Gen.mh(tr, select(:β, :i₀))` instead. The use of `global` in the loop is only necessary when running in a script; ideally, this would be run in a function."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "n_iter = 100000\nβ_mh = Vector{Real}(undef, n_iter)\ni₀_mh = Vector{Int}(undef, n_iter)\nscores = Vector{Float64}(undef, n_iter)\n(tr,) = Gen.generate(sir_markov_model, (l,fixed_args...), merge(observations, p))\nn_accept = 0\nfor i in 1:n_iter\n    global (tr, did_accept) = Gen.mh(tr, sir_proposal, ()) # Gen.mh(tr, select(:β, :i₀)) for untargeted\n    β_mh[i] = tr[:β]\n    i₀_mh[i] = tr[:i₀]\n    scores[i] = Gen.get_score(tr)\n    if did_accept\n        global n_accept += 1\n    end\nend;"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "We aim for about a 30% acceptance rate, which is a good rule of thumb for Metropolis-Hastings; after tweaking the proposal variances above, we arrive at a reasonable(ish) acceptance rate."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "acceptance_rate = n_accept/n_iter"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "pl_β_mh = histogram(β_mh, label=false, title=\"β\", ylabel=\"Density\", density=true, xrotation=45)\nvline!([sol[:β]], label=\"True value\")\npl_i₀_mh = histogram(i₀_mh, label=false, title=\"i₀\", ylabel=\"Density\", density=true, xrotation=45)\nvline!([sol[:i₀]], label=\"True value\")\nplot(pl_β_mh, pl_i₀_mh, layout=(1,2), plot_title=\"Metropolis-Hastings\")"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sequential Monte Carlo\n\nTo run Sequential Monte Carlo, we define a proposal kernel that will be used to rejuvenate the particles. This is a Metropolis-Hastings kernel, which we define as a function `kern` that takes a trace as an argument and returns a new trace and whether the move was accepted; in this case, we re-use the proposal from the Metropolis_Hastings MCMC above."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "kern(tr) = Gen.mh(tr, sir_proposal, ());"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "GenParticleFilters.jl defines a number of useful functions with which to build a particle filter. We define a function `particle_filter` that takes the observations, the number of particles, and an optional threshold for the effective sample size. We initialize the particle filter with the first observation, and then iterate across the remaining observations. If the effective sample size falls below the threshold, we resample the particles and rejuvenate them. We then update the filter state with the new observation at timestep `t`. Internally, Gen.jl runs the model at increasing values of time (hence the use of the number of steps as the first argument of the model), and fixes the parameters of the system up to that timepoint."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "function particle_filter(observations, n_particles, ess_thresh=0.5)\n    # Initialize particle filter with first observation\n    n_obs = length(observations)\n    obs_choices = [choicemap((:y, t) => observations[t]) for t=1:n_obs]\n    state = pf_initialize(sir_markov_model, (1,fixed_args...), obs_choices[1], n_particles)\n    # Iterate across timesteps\n    for t=2:n_obs\n        # Resample if the effective sample size is too low\n        if effective_sample_size(state) < ess_thresh * n_particles\n            # Perform residual resampling, pruning low-weight particles\n            pf_resample!(state, :residual)\n            # Rejuvenate particles\n            pf_rejuvenate!(state, kern, ())\n        end\n        # Update filter state with new observation at timestep t\n        # The following code explicitly allows for the number of timesteps to change\n        # while keeping the other arguments fixed\n        new_args = (t, fixed_args...)\n        argdiffs = (UnknownChange(), map(x -> NoChange(), new_args)...)\n        pf_update!(state, new_args, argdiffs, obs_choices[t])\n    end\n    return state\nend;"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "n_particles = 10000\nstate = particle_filter(Y, n_particles);"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "The state contains the traces of the particles, which we can use to calculate the effective sample size, as well as the (weighted) mean and standard deviation of the parameter estimates."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "effective_sample_size(state)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "mean(state, :β), sqrt(var(state, :β))"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "mean(state, :i₀), sqrt(var(state, :i₀))"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can extract the parameter estimates and weights from the state as follows."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "β_smc = getindex.(state.traces, :β)\ni₀_smc = getindex.(state.traces, :i₀)\nw = get_norm_weights(state);"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "The marginal histograms (using the weights `w`) can be plotted out as follows."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "pl_β_smc = histogram(β_smc, label=false, title=\"β\", ylabel=\"Density\", density=true, xrotation=45, weights=w)\nvline!([sol[:β]], label=\"True value\")\npl_i₀_smc = histogram(i₀_smc, label=false, title=\"i₀\", ylabel=\"Density\", density=true, xrotation=45, weights=w)\nvline!([sol[:i₀]], label=\"True value\")\nplot(pl_β_smc, pl_i₀_smc, layout=(1,2), plot_title=\"Sequential Monte Carlo\")"
      ],
      "metadata": {},
      "execution_count": null
    }
  ],
  "nbformat_minor": 2,
  "metadata": {
    "language_info": {
      "file_extension": ".jl",
      "mimetype": "application/julia",
      "name": "julia",
      "version": "1.10.3"
    },
    "kernelspec": {
      "name": "julia-1.10",
      "display_name": "Julia 1.10.3",
      "language": "julia"
    }
  },
  "nbformat": 4
}
