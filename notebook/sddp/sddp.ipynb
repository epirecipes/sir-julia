{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Optimal control of an SIR epidemic with a non-pharmaceutical intervention using SDDP.jl\nSean L. Wu (@slwu89) and Simon Frost (@sdwfrost), 2023-5-9\n\n## Introduction\n\n[SDDP.jl](https://odow.github.io/SDDP.jl/stable/) (stochastic dual dynamic programming) is a package designed\nto solve optimal policies in multi-stage (time or world state) linear programming problems with exogeneous stochasticity.\nWe can use it to optimize policy for a non-pharmaceutical intervention which decreses the transmission rate.\n\nBecause SDDP.jl solves an optimization problem for each node in a graph of nodes (which may represent the passage of time, or other changes\nin world state), the model we solve is a discretization of following ODEs ($\\upsilon$ is the intensity of intervention). \n\n$$\n\\begin{align*}\n\\dfrac{\\mathrm dS}{\\mathrm dt} &= -\\beta (1 - \\upsilon(t)) S I, \\\\\n\\dfrac{\\mathrm dI}{\\mathrm dt} &= \\beta (1 - \\upsilon(t)) S I - \\gamma I,\\\\ \n\\dfrac{\\mathrm dC}{\\mathrm dt} &= \\beta (1 - \\upsilon(t)) S I\\\\\n\\end{align*}\n$$\n\nThe minimization objective at each node (time point) is a linear combination of cumulative intervention applied,\nand cumulative cases. The total cumulative intervention force applied cannot exceed some maximum value.\nThe decision variable is the intensity of the intervention at each time point (node).\n\n## Libraries"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "using SDDP, JuMP, Ipopt, Plots;"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Parameters\n\nWe set the parameters, which includes the maximum intervention level at any node, `υ_max`, and the cost, which is the integral of the intervention level over time, `υ_total`."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "β = 0.5 # infectivity rate\nγ = 0.25 # recovery rate\nυ_max = 0.5 # maximum intervention\nυ_total = 10.0; # maximum cost"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Time domain\n\nWe set the time horizon to be long enough for the system to settle down to an equilibrium. We use a grid of timepoints fine enough to capture a wide variety of policy shapes, but coarse enough to keep the number of policy parameters to optimize low."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "tmax = 100.0\nδt = 1.0\nnsteps = Int(tmax / δt);"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initial conditions\n\nWe set the initial conditions for the proportion of susceptibles and infecteds."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "u0 = [0.99, 0.01]; # S,I"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model setup\n\nWe specify a model using `SDDP.LinearPolicyGraph`. Because the nodes in the policy graph represent the\npassage of time, we use a linear policy graph. We set the `optimizer` to the one from the `Ipopt`.\n\nWe set `S`, `I`, and `C` to be `SDDP.State` variables, meaning the values from the previous node in the policy\ngraph will be available to the current node. We specify 2 constraints on the intervention. While the second\nconstraint is mathematically the same as specifying `υ_cumulative.out ≤ υ_total` we must write it in\nthe form shown so that `υ` appears in the constraint.\n\nWe then set up the differences as non-linear expressions and the update rules as non-linear constraints.\nFinally, we use `@stageobjective` to set the minimization objective for this node to be a linear combination\nof total intervention pressure and cumulative cases."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "model = SDDP.LinearPolicyGraph(\n    stages = nsteps,\n    sense = :Min,\n    lower_bound = 0,\n    optimizer = Ipopt.Optimizer,\n) do sp, t\n\n    @variable(sp, 0 ≤ S, SDDP.State, initial_value = u0[1])\n    @variable(sp, 0 ≤ I, SDDP.State, initial_value = u0[2])\n    @variable(sp, 0 ≤ C, SDDP.State, initial_value = 0)\n\n    @variable(sp, 0 ≤ υ_cumulative, SDDP.State, initial_value = 0)\n    @variable(sp, 0 ≤ υ ≤ υ_max)\n\n    # constraints on control    \n    @constraint(sp, υ_cumulative.out == υ_cumulative.in + (δt * υ))\n    @constraint(sp, υ_cumulative.in + (δt * υ) ≤ υ_total)\n\n    # expressions to simplify the state updates\n    @NLexpression(sp, infection, (1-exp(-(1 - υ) * β * I.in * δt)) * S.in)\n    @NLexpression(sp, recovery, (1-exp(-γ*δt)) * I.in)\n\n    # state updating rules\n    @NLconstraint(sp, S.out == S.in - infection)\n    @NLconstraint(sp, I.out == I.in + infection - recovery)\n    @NLconstraint(sp, C.out == C.in + infection)\n\n    # linear weighting of objectives\n    @stageobjective(sp, υ_cumulative.out + 40*C.out)\n\nend;"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Running the model\n\nWe train the model for 100 iterations. SDDP.jl needs to iterate between forwards passes over the policy\ngraph where the policy is optimized given an approximation of the overall objective for each node,\nand backwards passes to improve the approximation."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "SDDP.train(model; iteration_limit = 50, print_level = 0);"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Plotting\n\nAfter the model has been trained, we can simulate from the model under the final optimal policy.\nThe second argument is the number of trajectories to draw (because the model is deterministic, a single\ntrajectory will suffice). The third argument is the variables to record during simulation."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "sims = SDDP.simulate(model, 1, [:S,:I, :C, :υ, :υ_cumulative]);"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can use the plotting utilities of SDDP.jl to show the optimal policy and state variables."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "Plots.plot(\n    SDDP.publication_plot(sims, title = \"S\") do data\n        return data[:S].out\n    end,\n    SDDP.publication_plot(sims, title = \"I\") do data\n        return data[:I].out\n    end,\n    SDDP.publication_plot(sims, title = \"C\") do data\n        return data[:C].out\n    end,\n    SDDP.publication_plot(sims, title = \"Control\") do data\n        return data[:υ]\n    end,\n    SDDP.publication_plot(sims, title = \"Cumulative control\") do data\n        return data[:υ_cumulative].out\n    end;\n    xlabel = \"Time\"\n)"
      ],
      "metadata": {},
      "execution_count": null
    }
  ],
  "nbformat_minor": 2,
  "metadata": {
    "language_info": {
      "file_extension": ".jl",
      "mimetype": "application/julia",
      "name": "julia",
      "version": "1.8.5"
    },
    "kernelspec": {
      "name": "julia-1.8",
      "display_name": "Julia 1.8.5",
      "language": "julia"
    }
  },
  "nbformat": 4
}
